{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to Process the Sensor Readings - ProcessSensorReadings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "This Script uses Spark Streaming to read Kafka topics as they come in and then insert them into a MySQL database.  There are two main methods:\n",
    "\n",
    "*  Read actual sensor readings:  Kafka Topic (LumbarSensorReadings) -> writeLumbarReadings -> MySQL table: SensorReadings\n",
    "\n",
    "*  Read Training sensor readings:  Kafka Topic (LumbarSensorTrainingReadings) -> writeLumbarTrainingReadings -> MySQL table:  SensorTrainingReadings\n",
    "\n",
    "This script requires the [JDBC Driver](http://dev.mysql.com/downloads/connector/j/) in order to connect to to a MySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "#custom modules\n",
    "import MySQLConnection\n",
    "\n",
    "\"\"\"\n",
    "IMPORTANT:  MUST use class paths when using spark-submit\n",
    "$SPARK_HOME/bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.6.2,mysql:mysql-connector-java:5.1.28 ProcessSensorReadings.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"writeLumbarReadings\" method takes the rdd received from Spark Streaming as an input.  It then extracts the JSON data and converts to a SQLContext dataframe.\n",
    "\n",
    "After this it creates a new column in the dataframe that contains the \"feature vector\" that will be used to predict the posture.\n",
    "\n",
    "The prediction process uses a model that is created and saved previously.  it uses the feature vector to predict the posture.\n",
    "\n",
    "Finally, the extra feature column is dropped and the final dataframe is inserted into the MySQL database using JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLumbarReadings(time, rdd):\n",
    "\ttry:\n",
    "\t\t# Convert RDDs of the words DStream to DataFrame and run SQL query\n",
    "\t\tconnectionProperties = MySQLConnection.getDBConnectionProps('/home/erik/mysql_credentials.txt')\n",
    "\t\tsqlContext = SQLContext(rdd.context)\n",
    "\t\tif rdd.isEmpty() == False:\n",
    "\t\t\tlumbarReadings = sqlContext.jsonRDD(rdd)\n",
    "\t\t\tlumbarReadingsIntermediate = lumbarReadings.selectExpr(\"readingID\",\"readingTime\",\"deviceID\",\"metricTypeID\",\"uomID\",\"actual.y AS actualYaw\",\"actual.p AS actualPitch\",\"actual.r AS actualRoll\",\"setPoints.y AS setPointYaw\",\"setPoints.p AS setPointPitch\",\"setPoints.r AS setPointRoll\")\n",
    "\t\t\tassembler = VectorAssembler(\n",
    "\t\t\t\t\t\tinputCols=[\"actualPitch\"], # Must be in same order as what was used to train the model.  Testing using only pitch since model has limited dataset.\n",
    "\t\t\t\t\t\toutputCol=\"features\")\n",
    "\t\t\tlumbarReadingsIntermediate = assembler.transform(lumbarReadingsIntermediate)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tpredictions = loadedModel.predict(lumbarReadingsIntermediate.map(lambda x: x.features))\n",
    "\t\t\tpredictionsDF = lumbarReadingsIntermediate.map(lambda x: x.readingID).zip(predictions).toDF([\"readingID\",\"positionID\"])\n",
    "\t\t\tcombinedDF = lumbarReadingsIntermediate.join(predictionsDF, lumbarReadingsIntermediate.readingID == predictionsDF.readingID).drop(predictionsDF.readingID)\n",
    "\t\t\t\n",
    "\t\t\tcombinedDF = combinedDF.drop(\"features\")\n",
    "\t\t\t\n",
    "\t\t\tcombinedDF.show()\n",
    "\n",
    "\n",
    "\t\t\tcombinedDF.write.jdbc(\"jdbc:mysql://localhost/biosensor\", \"SensorReadings\", properties=connectionProperties)\n",
    "\texcept:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"writeLumbarTrainingReadings\" method also accepts an RDD from Spark Streaming but does not need to do any machine learning processing since we already know the posture from the JSON data.\n",
    "\n",
    "Readings are simply transformed to a SQLContext dataframe and then inserted into the MySQL training readings table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLumbarTrainingReadings(time, rddTraining):\n",
    "\ttry:\n",
    "\t\t# Convert RDDs of the words DStream to DataFrame and run SQL query\n",
    "\t\tconnectionProperties = MySQLConnection.getDBConnectionProps('/home/erik/mysql_credentials.txt')\n",
    "\t\tsqlContext = SQLContext(rddTraining.context)\n",
    "\t\tif rddTraining.isEmpty() == False:\n",
    "\t\t\tlumbarTrainingReading = sqlContext.jsonRDD(rddTraining)\n",
    "\t\t\tlumbarTrainingReadingFinal = lumbarTrainingReading.selectExpr(\"deviceID\",\"metricTypeID\",\"uomID\",\"positionID\",\"actual.y AS actualYaw\",\"actual.p AS actualPitch\",\"actual.r AS actualRoll\",\"setPoints.y AS setPointYaw\",\"setPoints.p AS setPointPitch\",\"setPoints.r AS setPointRoll\")\n",
    "\t\t\tlumbarTrainingReadingFinal.write.jdbc(\"jdbc:mysql://localhost/biosensor\", \"SensorTrainingReadings\", properties=connectionProperties)\n",
    "\texcept:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main part of the script the machine learning model is loaded and then two Spark StreamingContexts are created to listen for either actual device readings or training readings.  The appropriate methods are then called upon receipt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tsc = SparkContext(appName=\"Process Lumbar Sensor Readings\")\n",
    "\tssc = StreamingContext(sc, 2) # 2 second batches\n",
    "\tloadedModel = RandomForestModel.load(sc, \"../machine_learning/models/IoTBackBraceRandomForest.model\")\n",
    "\n",
    "\t#Process Readings\n",
    "\tstreamLumbarSensor = KafkaUtils.createDirectStream(ssc, [\"LumbarSensorReadings\"], {\"metadata.broker.list\": \"localhost:9092\"})\n",
    "\tlineSensorReading = streamLumbarSensor.map(lambda x: x[1])\n",
    "\tlineSensorReading.foreachRDD(writeLumbarReadings)\n",
    "\t\n",
    "\t#Process Training Readings\n",
    "\tstreamLumbarSensorTraining = KafkaUtils.createDirectStream(ssc, [\"LumbarSensorTrainingReadings\"], {\"metadata.broker.list\": \"localhost:9092\"})\n",
    "\tlineSensorTrainingReading = streamLumbarSensorTraining.map(lambda x: x[1])\n",
    "\tlineSensorTrainingReading.foreachRDD(writeLumbarTrainingReadings)\n",
    "\n",
    "\t# Run and then wait for termination signal\n",
    "\tssc.start()\n",
    "\tssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
